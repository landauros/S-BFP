"""
æµè§ˆå™¨æŒ‡çº¹ç†µ (Entropy) åˆ†æè„šæœ¬
ç”¨äºåˆ†ææ”¶é›†çš„æµè§ˆå™¨æŒ‡çº¹æ•°æ®çš„åŒºåˆ†èƒ½åŠ›

æ ¸å¿ƒæ¦‚å¿µï¼š
- ç†µ (Entropy): è¡¡é‡ç‰¹å¾çš„åŒºåˆ†èƒ½åŠ›/ä¿¡æ¯é‡
- H = -Î£ p(x) * log2(p(x))
- ç†µè¶Šé«˜ = åŒºåˆ†èƒ½åŠ›è¶Šå¼º
"""

import json
import os
import math
import hashlib
from collections import Counter
from pathlib import Path
import pandas as pd

# æ•°æ®ç›®å½•
USERS_DIR = Path(r"C:\Users\W\Desktop\IEEE s&p\App_eng\App_eng\User_Manager\data\users")
RESULTS_DIR = Path(r"C:\Users\W\Desktop\IEEE s&p\App_eng\App_eng\results")


def calculate_entropy(values):
    """
    è®¡ç®—ç†µå€¼ (Shannon Entropy)
    H = -Î£ p(x) * log2(p(x))
    
    è¿”å›å€¼å•ä½: bits
    """
    if not values:
        return 0.0
    
    total = len(values)
    counter = Counter(values)
    
    entropy = 0.0
    for count in counter.values():
        if count > 0:
            p = count / total
            entropy -= p * math.log2(p)
    
    return entropy


def calculate_anonymity_set(values):
    """
    è®¡ç®—åŒ¿åé›†å¤§å° - æ¯ä¸ªå”¯ä¸€å€¼å¹³å‡æœ‰å¤šå°‘ç”¨æˆ·å…±äº«
    """
    if not values:
        return 0
    
    counter = Counter(values)
    unique_count = len(counter)
    total = len(values)
    
    # å¹³å‡åŒ¿åé›†å¤§å°
    avg_anonymity = total / unique_count if unique_count > 0 else 0
    return avg_anonymity


def calculate_uniqueness_rate(values):
    """
    è®¡ç®—å”¯ä¸€æ€§æ¯”ç‡ - æœ‰å¤šå°‘æ¯”ä¾‹çš„å€¼æ˜¯å”¯ä¸€çš„
    """
    if not values:
        return 0.0
    
    counter = Counter(values)
    unique_count = sum(1 for count in counter.values() if count == 1)
    return unique_count / len(values) * 100


def get_max_entropy(n_samples):
    """
    è®¡ç®—ç†è®ºæœ€å¤§ç†µ (æ‰€æœ‰å€¼éƒ½ä¸åŒæ—¶)
    H_max = log2(N)
    """
    if n_samples <= 1:
        return 0.0
    return math.log2(n_samples)


def load_user_fingerprints():
    """
    åŠ è½½æ‰€æœ‰ç”¨æˆ·çš„æŒ‡çº¹æ•°æ®
    é€‚åº”æ–°çš„æ•°æ®ç»“æ„
    """
    fingerprints = []
    
    for file_path in USERS_DIR.glob("*.json"):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                # æ„å»ºç»Ÿä¸€çš„æŒ‡çº¹å¯¹è±¡
                fp = {
                    'username': data.get('username', file_path.stem),
                    'source_file': file_path.name,
                }
                
                # æå–æŒ‡çº¹æ•°æ®
                if 'fingerprint' in data:
                    fp_data = data['fingerprint']
                    fp['fingerprint_hash'] = fp_data.get('hash', '')
                    fp['captured_at'] = fp_data.get('captured_at', '')
                    fp['user_agent'] = fp_data.get('user_agent', '')
                    fp['client_ip'] = fp_data.get('client_ip', '')
                    
                    # Canvas æ•°æ® (Base64 å›¾ç‰‡)
                    if 'canvas_data' in fp_data:
                        canvas = fp_data['canvas_data']
                        # ä½¿ç”¨å›¾ç‰‡æ•°æ®çš„å“ˆå¸Œä½œä¸ºç‰¹å¾
                        if isinstance(canvas, dict) and 'imageBase64' in canvas:
                            fp['canvas_hash'] = hashlib.sha256(
                                canvas['imageBase64'].encode()
                            ).hexdigest()[:16]
                        elif isinstance(canvas, str):
                            fp['canvas_hash'] = hashlib.sha256(
                                canvas.encode()
                            ).hexdigest()[:16]
                    
                # Audio åŸºçº¿
                if 'audio_baseline' in data:
                    fp['audio_baseline'] = data['audio_baseline']
                
                # Audio ç¨³å®šæ€§æµ‹è¯•
                if 'audio_stability' in data and len(data['audio_stability']) > 0:
                    stability = data['audio_stability'][0]
                    fp['audio_all_stable'] = stability.get('all_stable', False)
                    fp['audio_unique_hashes'] = len(stability.get('unique_hashes', []))
                    
                    # æå–éŸ³é¢‘é…ç½®
                    if 'runs' in stability and len(stability['runs']) > 0:
                        audio_config = stability['runs'][0].get('audioConfig', {})
                        fp['audio_sample_rate'] = audio_config.get('sampleRate', 0)
                
                # WebGL æ•°æ®
                if 'webgl_baseline' in data:
                    fp['webgl_baseline'] = data['webgl_baseline']
                
                if 'webgl_stability' in data and len(data['webgl_stability']) > 0:
                    webgl_stab = data['webgl_stability'][0]
                    fp['webgl_all_stable'] = webgl_stab.get('all_stable', False)
                
                # WebGL2 æ•°æ®
                if 'webgl2_baseline' in data:
                    fp['webgl2_baseline'] = data['webgl2_baseline']
                
                # Canvas æ•°æ®
                if 'canvas_baseline' in data:
                    fp['canvas_baseline'] = data['canvas_baseline']
                
                fingerprints.append(fp)
                
        except Exception as e:
            print(f"Error loading {file_path}: {e}")
    
    return fingerprints


def extract_feature_values(fingerprints, feature_key):
    """
    ä»æŒ‡çº¹æ•°æ®ä¸­æå–ç‰¹å®šç‰¹å¾çš„æ‰€æœ‰å€¼
    """
    values = []
    
    for fp in fingerprints:
        value = fp.get(feature_key)
        if value is not None:
            if isinstance(value, (dict, list)):
                value = json.dumps(value, sort_keys=True)
            else:
                value = str(value)
            values.append(value)
    
    return values


def analyze_single_feature(fingerprints, feature_name, feature_key):
    """
    åˆ†æå•ä¸ªç‰¹å¾çš„ç†µ
    """
    values = extract_feature_values(fingerprints, feature_key)
    
    if not values:
        return None
    
    n_samples = len(values)
    n_unique = len(set(values))
    entropy = calculate_entropy(values)
    max_entropy = get_max_entropy(n_samples)
    normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0
    uniqueness_rate = calculate_uniqueness_rate(values)
    anonymity_set = calculate_anonymity_set(values)
    
    return {
        'feature': feature_name,
        'n_samples': n_samples,
        'n_unique_values': n_unique,
        'entropy_bits': round(entropy, 4),
        'max_entropy_bits': round(max_entropy, 4),
        'normalized_entropy': round(normalized_entropy, 4),
        'uniqueness_rate_%': round(uniqueness_rate, 2),
        'avg_anonymity_set': round(anonymity_set, 2),
        'distinguishing_power': round(2 ** entropy, 2)  # ç†è®ºä¸Šå¯ä»¥åŒºåˆ†å¤šå°‘ç”¨æˆ·
    }


def get_feature_list():
    """
    å®šä¹‰è¦åˆ†æçš„ç‰¹å¾åˆ—è¡¨
    (ç‰¹å¾åç§°, æ•°æ®key)
    """
    return [
        # åŸºç¡€ä¿¡æ¯
        ("User Agent", "user_agent"),
        ("Client IP", "client_ip"),
        
        # ä¸»æŒ‡çº¹å“ˆå¸Œ
        ("Fingerprint Hash", "fingerprint_hash"),
        
        # Canvas æŒ‡çº¹
        ("Canvas Hash", "canvas_hash"),
        ("Canvas Baseline", "canvas_baseline"),
        
        # WebGL æŒ‡çº¹
        ("WebGL Baseline", "webgl_baseline"),
        ("WebGL Stable", "webgl_all_stable"),
        ("WebGL2 Baseline", "webgl2_baseline"),
        
        # Audio æŒ‡çº¹
        ("Audio Baseline", "audio_baseline"),
        ("Audio Stable", "audio_all_stable"),
        ("Audio Sample Rate", "audio_sample_rate"),
    ]


def analyze_combined_features(fingerprints, feature_combinations):
    """
    åˆ†æç»„åˆç‰¹å¾çš„ç†µ
    """
    results = []
    
    for combo_name, feature_keys in feature_combinations:
        combined_values = []
        
        for fp in fingerprints:
            combo_value = []
            for key in feature_keys:
                value = fp.get(key)
                if value is not None:
                    combo_value.append(str(value))
            
            if combo_value:
                combined_values.append('|'.join(combo_value))
        
        if combined_values:
            n_samples = len(combined_values)
            n_unique = len(set(combined_values))
            entropy = calculate_entropy(combined_values)
            max_entropy = get_max_entropy(n_samples)
            
            results.append({
                'combination': combo_name,
                'n_samples': n_samples,
                'n_unique_values': n_unique,
                'entropy_bits': round(entropy, 4),
                'max_entropy_bits': round(max_entropy, 4),
                'normalized_entropy': round(entropy / max_entropy if max_entropy > 0 else 0, 4),
            })
    
    return results


def print_value_distribution(fingerprints, feature_key, feature_name, top_n=10):
    """
    æ‰“å°ç‰¹å¾å€¼çš„åˆ†å¸ƒæƒ…å†µ
    """
    values = extract_feature_values(fingerprints, feature_key)
    if not values:
        return
    
    counter = Counter(values)
    total = len(values)
    
    print(f"\n  {feature_name} å€¼åˆ†å¸ƒ (Top {min(top_n, len(counter))}):")
    for value, count in counter.most_common(top_n):
        # æˆªæ–­è¿‡é•¿çš„å€¼
        display_value = value[:50] + "..." if len(value) > 50 else value
        percentage = count / total * 100
        print(f"    - '{display_value}': {count} ({percentage:.1f}%)")


def main():
    print("=" * 70)
    print("ğŸ”¬ æµè§ˆå™¨æŒ‡çº¹ç†µ (Entropy) åˆ†ææŠ¥å‘Š")
    print("=" * 70)
    print("\nğŸ“– ç†µ(Entropy)æ¦‚å¿µè¯´æ˜:")
    print("   - ç†µè¡¡é‡ç‰¹å¾çš„åŒºåˆ†èƒ½åŠ›/ä¿¡æ¯é‡")
    print("   - å…¬å¼: H = -Î£ p(x) Ã— logâ‚‚(p(x))")
    print("   - ç†µè¶Šé«˜ = å€¼è¶Šåˆ†æ•£ = åŒºåˆ†èƒ½åŠ›è¶Šå¼º")
    print("   - æœ€å¤§ç†µ = logâ‚‚(N), Nä¸ºæ ·æœ¬æ•°")
    
    # åŠ è½½æ•°æ®
    fingerprints = load_user_fingerprints()
    print(f"\nğŸ“Š åŠ è½½äº† {len(fingerprints)} ä¸ªç”¨æˆ·æŒ‡çº¹æ•°æ®\n")
    
    if len(fingerprints) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æŒ‡çº¹æ•°æ®!")
        return
    
    # æ˜¾ç¤ºåŠ è½½çš„ç”¨æˆ·
    print("ç”¨æˆ·åˆ—è¡¨:")
    for fp in fingerprints:
        print(f"  - {fp['username']} ({fp['source_file']})")
    
    # 1. å•ä¸ªç‰¹å¾ç†µåˆ†æ
    print("\n" + "-" * 70)
    print("ğŸ“ˆ å•ä¸ªç‰¹å¾ç†µåˆ†æ")
    print("-" * 70)
    
    feature_results = []
    for feature_name, feature_key in get_feature_list():
        result = analyze_single_feature(fingerprints, feature_name, feature_key)
        if result:
            feature_results.append(result)
    
    # æŒ‰ç†µå€¼æ’åº
    feature_results.sort(key=lambda x: x['entropy_bits'], reverse=True)
    
    # æ‰“å°ç»“æœè¡¨æ ¼
    print(f"\n{'ç‰¹å¾åç§°':<20} {'ç†µ(bits)':<10} {'å”¯ä¸€å€¼':<8} {'å”¯ä¸€ç‡%':<10} {'åŒºåˆ†åŠ›':<10} {'å½’ä¸€åŒ–ç†µ':<10}")
    print("-" * 70)
    
    for r in feature_results:
        print(f"{r['feature']:<20} {r['entropy_bits']:<10} {r['n_unique_values']:<8} {r['uniqueness_rate_%']:<10} {r['distinguishing_power']:<10} {r['normalized_entropy']:.4f}")
    
    # 2. æ˜¾ç¤ºå…³é”®ç‰¹å¾çš„å€¼åˆ†å¸ƒ
    print("\n" + "-" * 70)
    print("ğŸ“Š ç‰¹å¾å€¼åˆ†å¸ƒè¯¦æƒ…")
    print("-" * 70)
    
    key_features = [
        ("fingerprint_hash", "Fingerprint Hash"),
        ("audio_baseline", "Audio Baseline"),
        ("webgl_baseline", "WebGL Baseline"),
        ("canvas_baseline", "Canvas Baseline"),
        ("user_agent", "User Agent"),
    ]
    
    for key, name in key_features:
        print_value_distribution(fingerprints, key, name, top_n=5)
    
    # 3. ç»„åˆç‰¹å¾åˆ†æ
    print("\n" + "-" * 70)
    print("ğŸ”— ç»„åˆç‰¹å¾ç†µåˆ†æ")
    print("-" * 70)
    
    combinations = [
        ("Audio + WebGL", ["audio_baseline", "webgl_baseline"]),
        ("Audio + WebGL + WebGL2", ["audio_baseline", "webgl_baseline", "webgl2_baseline"]),
        ("Canvas + WebGL + Audio", ["canvas_baseline", "webgl_baseline", "audio_baseline"]),
        ("Complete Fingerprint", ["fingerprint_hash"]),
    ]
    
    combo_results = analyze_combined_features(fingerprints, combinations)
    
    if combo_results:
        print(f"\n{'ç»„åˆåç§°':<25} {'ç†µ(bits)':<10} {'å”¯ä¸€å€¼':<8} {'å½’ä¸€åŒ–ç†µ':<10}")
        print("-" * 60)
        
        for r in combo_results:
            print(f"{r['combination']:<25} {r['entropy_bits']:<10} {r['n_unique_values']:<8} {r['normalized_entropy']:.4f}")
    
    # 4. ç†µåˆ†ææ€»ç»“
    print("\n" + "=" * 70)
    print("ğŸ“‹ åˆ†ææ€»ç»“")
    print("=" * 70)
    
    n_samples = len(fingerprints)
    max_possible_entropy = get_max_entropy(n_samples)
    
    print(f"\næ ·æœ¬æ•°é‡: {n_samples}")
    print(f"ç†è®ºæœ€å¤§ç†µ: {max_possible_entropy:.4f} bits")
    print(f"(å³å¦‚æœæ¯ä¸ªç”¨æˆ·éƒ½å®Œå…¨å”¯ä¸€ï¼Œå¯ä»¥è¾¾åˆ°çš„æœ€å¤§ç†µ)")
    
    if feature_results:
        top_features = [r for r in feature_results if r['entropy_bits'] > 0][:5]
        if top_features:
            print(f"\nğŸ† ç†µå€¼æœ€é«˜çš„ç‰¹å¾ (åŒºåˆ†èƒ½åŠ›æœ€å¼º):")
            for i, r in enumerate(top_features, 1):
                print(f"  {i}. {r['feature']}: {r['entropy_bits']:.4f} bits (å¯åŒºåˆ† {r['distinguishing_power']} ç”¨æˆ·)")
    
    # 5. ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    output_dir = Path(r"C:\Users\W\Desktop\IEEE s&p\App_eng\App_eng\reports3")
    output_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜ä¸ºCSV
    if feature_results:
        df = pd.DataFrame(feature_results)
        csv_path = output_dir / "entropy_analysis_report.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"\nğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {csv_path}")
    
    # 6. è®¡ç®—ç†è®ºè¯†åˆ«èƒ½åŠ›
    print("\n" + "-" * 70)
    print("ğŸ¯ æŒ‡çº¹è¯†åˆ«èƒ½åŠ›ä¼°ç®—")
    print("-" * 70)
    
    # å‡è®¾ç‰¹å¾ç‹¬ç«‹ï¼Œæ€»ç†µ = å„ç‰¹å¾ç†µä¹‹å’Œ (ä¸Šç•Œä¼°è®¡)
    # æ’é™¤é‡å¤çš„ç‰¹å¾ (å¦‚ fingerprint_hash å·²åŒ…å«å…¶ä»–)
    independent_features = ['audio_baseline', 'webgl_baseline', 'canvas_baseline']
    total_entropy_upper = sum(
        r['entropy_bits'] for r in feature_results 
        if r['feature'].lower().replace(' ', '_') in [f.replace('_', ' ').lower() for f in independent_features]
        and r['entropy_bits'] > 0
    )
    
    # å®é™…æ€»ç†µ (ç”¨å®Œæ•´å“ˆå¸Œ)
    full_hash_result = next((r for r in feature_results if r['feature'] == 'Fingerprint Hash'), None)
    actual_entropy = full_hash_result['entropy_bits'] if full_hash_result else 0
    
    print(f"\nç‹¬ç«‹ç‰¹å¾ç†µä¹‹å’Œ: {total_entropy_upper:.4f} bits")
    print(f"å®Œæ•´æŒ‡çº¹ç†µ: {actual_entropy:.4f} bits")
    
    if actual_entropy > 0:
        distinguishable = 2 ** actual_entropy
        print(f"å¯åŒºåˆ†çš„ç†è®ºç”¨æˆ·æ•°: 2^{actual_entropy:.2f} â‰ˆ {distinguishable:.0f} ç”¨æˆ·")
    
    # 7. å»ºè®®
    print("\n" + "-" * 70)
    print("ğŸ’¡ æ”¹è¿›å»ºè®®")
    print("-" * 70)
    
    print(f"""
å½“å‰æ•°æ®é‡: {n_samples} ç”¨æˆ· (è¾ƒå°‘)

ä¸ºäº†æ›´å‡†ç¡®çš„ç†µåˆ†æï¼Œå»ºè®®:
1. æ”¶é›†æ›´å¤šæ ·æœ¬ (è‡³å°‘ 100+ ç”¨æˆ·)
2. è¦†ç›–ä¸åŒæµè§ˆå™¨/è®¾å¤‡ç»„åˆ
3. åˆ†æè·¨æ—¶é—´çš„ç¨³å®šæ€§

ç†µåˆ†æçš„æ„ä¹‰:
- é«˜ç†µç‰¹å¾ â†’ é€‚åˆä½œä¸ºä¸»è¦è¯†åˆ«ä¾æ®
- ä½ç†µç‰¹å¾ â†’ å¯ä½œä¸ºè¾…åŠ©ç‰¹å¾
- ç»„åˆå¤šä¸ªç‰¹å¾å¯ä»¥å¢åŠ æ€»ç†µ

å‚è€ƒæ–‡çŒ®æ¨è:
- Eckersley (2010): "How Unique Is Your Web Browser?"
- Laperdrix et al.: "Browser Fingerprinting: A Survey"
""")
    
    print("\n" + "=" * 70)
    print("åˆ†æå®Œæˆ!")
    print("=" * 70)


if __name__ == "__main__":
    main()
